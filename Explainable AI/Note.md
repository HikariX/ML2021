# Explainable AI

**为什么要追求可解释性**

随着机器学习/深度学习模型的性能提升，其结果背后的产生机制也受到了各种质疑。这就好像传说有一匹马能够算出数学算式并跺脚对应的次数。而有人去测试发现，马不过是观察周围人的反应来停止跺脚，以获得奖励罢了，他并不了解算数背后的逻辑。

<img src="image-20210527212803282.png" alt="image-20210527212803282" style="zoom:25%;" />

同样地，如果一个机器学习模型是通过莫名其妙的方式得到正确的答案，那么这些答案的解释度便会存疑。在面对法律判决、疾病诊断、自动驾驶等任务的时候，模型对于结果产生的合理机制比结果的正确性更加让人关注。

***切莫因噎废食***

当然，不能因为模型的解释度差就弃之不用。毕竟许多模型已经能够做到非常好的结果了。而目前具有可解释性的模型中，线性模型是不够强大的，性能远落后于不可解释的深度学习模型。

<img src="image-20210527213018852.png" alt="image-20210527213018852" style="zoom:25%;" />

此外，决策树模型的解释性倒是不错。但是实际应用中为了保证效果，使用的是多层、多棵决策树（Ensemble），面对种类繁多的子树，做出解释同样不容易。

<img src="image-20210527213609306.png" alt="image-20210527213609306" style="zoom:25%;" />

**可解释模型的目标是什么**

模型的可解释性是一个标准不一的目标。公说公有理，婆说婆有理，所以只要模型的某种结果在某种程度上让人信服就足够了。核心是让人信服，而不是整个模型得多“透明”。

### Explainable ML

一般来说，可解释模型分为两类：**local explanation**与**global explanation**。

<img src="image-20210527214003456.png" alt="image-20210527214003456" style="zoom:25%;" />

拿关于猫的图片分类任务举例。“Local”关心的是，为什么当前的某张图片是或不是猫，也就是关心模型对某个具体数据的判定依据。而“global”高于数据本身。它更关心，对于训练好的模型权重，猫的判定标准是什么。

#### Local Explanation

**Patching**

通过在某张图片上加上色块，再观察图片分类的结果，就可以确定计算机认为图片的那些位置比较重要。

<img src="截屏2021-05-27 21.51.05.png" alt="截屏2021-05-27 21.51.05" style="zoom:25%;" />

上图为某实验结果。第一行为图片，第二行为预测结果。第二行的对应位置为在第一行放色块后模型的预测准确度，红色为高，蓝色为低。显而易见，对预测狗的任务而言，用色块遮住狗的身子不大影响准确率，但遮住脸会带来准确率下降；而识别轮胎的任务里就不能遮住轮胎等。所以说图像识别模型在某种程度上是具有可解释性的。

**Saliency Map**

另一种方法是对某些数据做出微小扰动，观察最终损失函数的变化情况，后者与前者的比值越大，就说明改变这部分数据会让模型结果变化明显。通过这一思路可以画出如下图片：

<img src="image-20210527215403608.png" alt="image-20210527215403608" style="zoom:25%;" />

图中白色点代表比值高的区域。我们惊喜地发现，三张识别动物的图片中，白色点均分布在动物区域，而不是背景中，这也能说明图像识别模型的可解释性。

**限制**

有些图片的识别难度较大，产生的saliency map是模糊的，有人提出可以随机添加不同的噪声，形成不同的图片，再取平均，从而消除无关内容对解释性的影响：

<img src="image-20210527220451567.png" alt="image-20210527220451567" style="zoom:25%;" />

另一个问题是梯度饱和。用一个直观的例子来理解，如果大象的鼻子长到一定程度，那么它再长也只是大象，所以鼻子长度的变化并不会对“梯度”产生影响，这时候所谓的梯度对重要性的衡量效果就不够准确了。

<img src="image-20210527220641348.png" alt="image-20210527220641348" style="zoom:25%;" />

**网络如何处理输入的数据**

早在2012年，Hinton等人就尝试将语音信号的MFCC图通过低维映射转为二维图像，发现不同的人（图上的不同颜色）说同样的一系列内容，产生的特征五花八门：画出的图像毫无重叠。但是经过多层神经网络后，同样内容的不同语音数据隐变量就被展现到了相近的位置，这也说明这些数据背后拥有共性。

<img src="image-20210527220852726.png" alt="image-20210527220852726" style="zoom:25%;" />

类似地，对一个多层的语音-文字感知任务，可以抽取其中间得到的embedding，丢到文字-语音模型中，研究其生成的语音内容。随着抽取信息的层数增加，计算机也许就得到了语音数据背后的特征。面对某人的言语，反向得到的语音数据也许听不出说话者是谁。换言之，通过这种方法，可以研究神经网络是怎么“听”到声音，如何处理数据的。

<img src="image-20210527222026411.png" alt="image-20210527222026411" style="zoom:25%;" />