# Adversarial Attack

**动机**

人类希望通过自身的经验骗过机器学习模型，因此在一些简单任务上开始尝试修改数据骗过机器。但对于复杂的任务，显然需要通过某些训练方式来完成这一欺骗过程。

### 如何攻击

#### 实例介绍

以图片分类的攻击为例，在每一元素位置加上微小扰动后，送入原始的分类模型，有可能让它所对应的类别发生变化。以下为示意图。

<img src="image-20210519154418549.png" alt="image-20210519154418549" style="zoom:25%;" />

该图中，原本的图片属于tiger cat类，但通过添加扰动后，会成为其他类。当然，这种扰动十分明显。我们在研究攻击时，自然会希望能做到不明显的攻击，即骗过人眼。以下为另一个例子。

<img src="image-20210519154659530.png" alt="image-20210519154659530" style="zoom:25%;" />

可以看到，两张在人眼分辨上几乎相同的图片，前者被分类为tiger cat，后者却是star fish（并且分数高的吓人）。它们之间的差距放大五十倍后呈现出来是一副奇怪的马赛克。

以上的攻击看起来是毫无逻辑的，而还有一些攻击是可以理解的。例如某些人眼可识别的噪声，通过让图片变模糊，及其会把猫看成更毛茸茸的波斯猫，或者把猫当作模糊的火焰，背景当作壁炉...

<img src="image-20210519154940755.png" alt="image-20210519154940755" style="zoom:25%;" />

#### 攻击方法

攻击的目标是在某个训练好的分类器上从输入图片$\boldsymbol{x}_0$得到对应的能骗过网络的输出图片$\boldsymbol{x}$，则最主要的目标就是让输出的类别$\boldsymbol{y}$和真实类别$\hat{\boldsymbol{y}}$越远越好。将其定义为一个损失函数即可。注意，为了实现前述的人眼看不出的变化，实际上这一过程需要一个约束：$d(\boldsymbol{x}_0, \boldsymbol{x})\le\epsilon$（图中蓝框部分没显示）。

<img src="image-20210519155157102.png" alt="image-20210519155157102" style="zoom:25%;" />

这种目标输出和原始输出的近距离约束需要通过domain knowledge来完成。例如图像任务，我们通过衡量L2范数和无穷范数来判定$d(\boldsymbol{x}_0, \boldsymbol{x})$的大小。

<img src="image-20210519155649297.png" alt="image-20210519155649297" style="zoom:25%;" />

观察图中的马赛克，右上为四个像素都变化一点，右下为绿色格像素变化较多。但它们的L2范数完全相同，右下者无穷范数更大。从人眼角度，对右下的变化更为敏感。所以使用捕捉这一变化的无穷范数作为距离约束就可以保证输出和输入数据的接近。

与梯度下降类似，但此时固定网络参数，更新输入数值$\boldsymbol{x}$，即损失函数L对$\boldsymbol{x}$求偏导。注意对于约束$d$，在梯度更新的时候判断是否超限，若超限则在相同方向上“往回拉”一段即可。通过如此迭代，可以找到效果最好的输出数值。

<img src="image-20210519160248148.png" alt="image-20210519160248148" style="zoom:25%;" />

从约束$d$的特殊性出发，有人提出，不如一次迭代来解决攻击问题：

<img src="image-20210519160618698.png" alt="image-20210519160618698" style="zoom:25%;" />

通过让下降步长$\eta=\epsilon$，更新的梯度为原梯度方向的符号值，则每次更新后的结果一定会是无穷范数限制的高维多面体顶点。如此，在不违反限制的情况下又让输出数值有了最大的偏移，通过一次迭代就能完成。当然这一符号梯度的方法也可以加上多次迭代和“回拉”，以达到更佳的攻击效果。