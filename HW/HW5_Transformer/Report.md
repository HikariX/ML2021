# Report

### 作业心得

该作业为使用Encoder-Decoder进行英-中翻译任务，主要考量该架构的熟悉程度，以及NLP中的back-translation（BT）这一半监督学习技巧。

作业代码基本如助教所示，无需过多更改。simple baseline为跑完代码，medium为将模型架构切换为transformer，strong为使用BT方法+transformer。因时间紧和学习需要，我直接进行strong baseline目标的实现。

训练前，需要从网络下载数据并进行句子分解，产生单词、token等子单元。接下来将这些数据处理为二进制数据，供fairseq这一代码库进行网络模型训练。该过程在代码中体现清晰。我主要针对BT的过程进行处理。该任务为英-中翻译，所以BT需要从额外的中文语料产生英文翻译，再将这一对数据作为训练数据。因此，首先需要将网络模型进行反向训练，使其成为一个中-英翻译器。通过调节训练参数，模型可以读取准备好的数据资料并自动完成中-英训练。接下来，我们将下载的额外中文数据进行和先前相同的预处理后，送入网络进行预测。该过程由generate_prediction函数完成，产生一个prediction.txt文件。助教也提供了从该文件读取结果整理为英文训练资料的代码。然后对于产生的英文资料，同样进行tokenize等操作，最终我们额外下载的中文数据就变成了新的英-中资料。这就是BT过程。最后将这些资料整合到原始训练数据中，再调节参数让网络的训练方向为英-中，我们的模型就可以完成加入额外资料的训练。全过程通过ipynb文件的流程指引已十分清晰。上传的作业代码可能有运行问题，建议用colab进行分块分步骤地执行。

另外，在训练中我发现新产生的数据叫做train1，而原始的训练数据为train。但是fairseq库的数据读取过程能够自动整合这些数据。好奇之下我去查阅了源代码，发现该读取过程如下：

<img src="截屏2021-05-07 21.06.38.png" alt="截屏2021-05-07 21.06.38" style="zoom:100%;" />

这里的split就是train、valid、test等名字。而itertool类似于从0开始的计数器。所以该代码前两行的操作为从0开始往split后头增加编号，如果编号0，那就是train，否则是train1、train2等。这样一来，它就完成了自动合并不同训练集的操作。

### 训练日志

05.06

运行代码，阅读理解相关数据处理过程，尝试BT过程的数据生成操作。

05.07

初步完成BT过程，但产生预测资料的代码存在问题。

05.08

Debug成功，可以产生正确的预测资料。

完成BT后，使用新的数据进行合并训练，到第15epoch时BLEU为23+。Public的medium baseline为24.04，strong baseline为29.32。因此可以发现随着训练的增加，极有可能达到这一目标。